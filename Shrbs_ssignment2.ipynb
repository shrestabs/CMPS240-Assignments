{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIzIeC5kyXGS"
   },
   "source": [
    "# CMPS 240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "3UHcv0loyXGU",
    "nbgrader": {
     "checksum": "b87060785d34c61f42c2a758f71326cb",
     "grade": false,
     "grade_id": "cell-d032f2bff8ae7ea7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Due March 3, 2019 11:59**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "gUeVUWj4yXGV",
    "nbgrader": {
     "checksum": "bd8f8c01f8a1aebf2909e99624a3c543",
     "grade": false,
     "grade_id": "cell-fd085a91544fb57f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "Consider the following constraint satisfaction problem. A graph has nodes of the following types:\n",
    "- Triangle\n",
    "- Circle\n",
    "- Square\n",
    "- Hexagon\n",
    "- Pentagon\n",
    "\n",
    "Each node has a domain of {1, 2, ..., 9}.\n",
    "\n",
    "Each node type as the following constraints on its value:\n",
    "- Triangle - The leftmost digit of the product of all of its neightbors\n",
    "- Square - The rightmost digit of of the product of all its neighbors\n",
    "- Hexagon - The leftmost digit of the sum of all its neighbors\n",
    "- Pentagon - The rightmost digit of the sum of all its neighbors\n",
    "- Circle - No contraints\n",
    "\n",
    "Complete the function defined below:\n",
    "\n",
    "```python\n",
    "def solve_csp(nodes, arcs, max_steps):\n",
    "    \"\"\"\n",
    "    This function solves the csp using the MinConflicts Search\n",
    "    Algorithm.\n",
    "\n",
    "    INPUTS:\n",
    "    nodes:      a list of letters that indicates what type of node it is,\n",
    "                the index of the node in the list indicates its id\n",
    "                letters = {C, T, S, P, H}\n",
    "    arcs:       a list of tuples that contains two numbers, indicating the \n",
    "                IDS of the nodes the arc connects. \n",
    "    max_steps:  max number of steps to make before giving up\n",
    "\n",
    "    RETURNS: a list of values for the soltiion to the CSP where the \n",
    "             index of the value correxponds the the value for that\n",
    "             given node.\n",
    "    \"\"\"\n",
    "    node_values = []\n",
    "\n",
    "    return node_values\n",
    "```\n",
    "\n",
    "As a reminder here is the pseudo code for the Min-Conflicts search algorithm:\n",
    "\n",
    "![minconflicts](https://docs.google.com/drawings/d/e/2PACX-1vTIdOyAKDEoK6evNWQBkx9X5kl2I7GLaUkE9TdFDRqyyNFiHeFDrA-Sm7sLob2wMSzoBk_cliRhs8PY/pub?w=927&amp;h=474)\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- It's possible that you won't converge to a solution in a single run. Try a few runs to see if you get to a solution.\n",
    "- The example is to show you what a problem looks like, I will test/grade your program on different examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R-k48OukWL1q"
   },
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJGGenj1Ackv"
   },
   "outputs": [],
   "source": [
    "import random as random\n",
    "import numpy as numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "7Qmos8WvyXGW",
    "nbgrader": {
     "checksum": "b43c1d03ed4c4ee88edd859216f7d25a",
     "grade": true,
     "grade_id": "cell-c95dad2f1ac8adc2",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "   \"\"\"\n",
    "    This function solves the csp using the MinConflicts Search\n",
    "    Algorithm.\n",
    "\n",
    "    INPUTS:\n",
    "    nodes:      a list of letters that indicates what type of node it is,\n",
    "                the index of the node in the list indicates its id\n",
    "                letters = {C, T, S, P, H}. From input\n",
    "    node_values: current assignments\n",
    "    adjmat     : adjacency matrix\n",
    "    iternode   : this iteration node\n",
    "    \n",
    "    RETURNS: The new value to be assigned to the node under consideration\n",
    "   \"\"\"\n",
    "   def conflicts_util(nodes, node_values, adjmat, iternode):\n",
    "      listindex = []\n",
    "      listnum = []\n",
    "      for i in range(1,10):\n",
    "        node_values[iternode]=i\n",
    "        listindex.append(i);\n",
    "        listnum.append(len(conflicts_count(nodes,node_values,adjmat)))\n",
    "      min_num = min(listnum);\n",
    "      minlist=[i for i, j in zip(listindex, listnum) if j == min_num]\n",
    "      randomindex = random.choice(minlist)\n",
    "      return randomindex\n",
    "  \n",
    "   def conflicts_count(nodes, node_values, adjmat):    \n",
    "      \"\"\"\n",
    "      This function conflicts_count looks at the constraints and updates the val\n",
    "      for the nodes\n",
    "      INPUTS:\n",
    "      nodes      : a list of letters that indicates what type of node it is,\n",
    "                   the index of the node in the list indicates its id\n",
    "                letters = {C, T, S, P, H}. From input\n",
    "      node_values: current assignments\n",
    "      adjmat     : adjacency matrix\n",
    "      \n",
    "      RETURNS    : new set of conflict list after new assignments \n",
    "      \"\"\"\n",
    "      update_list=[]\n",
    "      for node in nodes:\n",
    "        if node == 'T':\n",
    "          #Triangle - The leftmost digit of the product of all of its neightbors\n",
    "          i = nodes.index(node)\n",
    "          list2=[node_values[x] for x in adjmat[i]] \n",
    "          digit = numpy.prod(list2)\n",
    "          digit = int(str(digit)[0])\n",
    "          update_list.insert(i,digit)\n",
    "          \n",
    "        if node == 'S':\n",
    "          #Square - The rightmost digit of of the product of all its neighbors\n",
    "          i = nodes.index(node)\n",
    "          list2=[node_values[x] for x in adjmat[i]] \n",
    "          digit = numpy.prod(list2)\n",
    "          digit = digit %10\n",
    "          if(digit != 0):\n",
    "            update_list.insert(i,digit)\n",
    "          else:\n",
    "            continue\n",
    "            \n",
    "        if node == 'H':\n",
    "          #Hexagon - The leftmost digit of the sum of all its neighbors\n",
    "          i = nodes.index(node)\n",
    "          list2=[node_values[x] for x in adjmat[i]] \n",
    "          digit = sum(list2)\n",
    "          digit = int(str(digit)[0])\n",
    "          if(digit != 0):\n",
    "            update_list.insert(i,digit)\n",
    "          else:\n",
    "            continue\n",
    "            \n",
    "        if node == 'P':\n",
    "          #Pentagon - The rightmost digit of the sum of all its neighbors\n",
    "          i = nodes.index(node)\n",
    "          list2=[node_values[x] for x in adjmat[i]] \n",
    "          rdigit = sum(list2)\n",
    "          rdigit = rdigit %10\n",
    "          update_list.insert(i,rdigit)\n",
    "          \n",
    "        if node == 'C':\n",
    "          #Circle - No contraints\n",
    "          i = nodes.index(node)\n",
    "          val = node_values[i]\n",
    "          update_list.insert(i,val)\n",
    "      \n",
    "      temp = []\n",
    "      for i in range(0,len(node_values)):\n",
    "        for j in range(0,len(update_list)):\n",
    "          if(i==j and node_values[i] != update_list[j] ):\n",
    "            temp.append(i)\n",
    "      #print(\"temp is \",temp)\n",
    "      return (temp)\n",
    "  \n",
    "  \n",
    "def solve_csp(nodes, arcs, max_steps):\n",
    "    \"\"\"\n",
    "    This function solves the csp using the MinConflicts Search\n",
    "    Algorithm.\n",
    "\n",
    "    INPUTS:\n",
    "    nodes:      a list of letters that indicates what type of node it is,\n",
    "                the index of the node in the list indicates its id\n",
    "                letters = {C, T, S, P, H}\n",
    "    arcs:       a list of tuples that contains two numbers, indicating the \n",
    "                IDS of the nodes the arc connects. \n",
    "    max_steps:  max number of steps to make before giving up\n",
    "\n",
    "    RETURNS: a list of values for the soltiion to the CSP where the \n",
    "             index of the value correxponds the the value for that\n",
    "             given node.\n",
    "    \"\"\"\n",
    "    node_values = []\n",
    "    n = len(nodes)\n",
    "    #create empty 2D matrix\n",
    "    adjmat = [[] for _ in range(n)]\n",
    "    \n",
    "    #create adj matrix\n",
    "    for node in nodes:\n",
    "      i = nodes.index(node)\n",
    "      for arc in arcs:\n",
    "        if(i in arc):\n",
    "          adjmat[i].append(arc[0])\n",
    "          adjmat[i].append(arc[1])\n",
    "       \n",
    "      adjmat[i]=set(adjmat[i])\n",
    "      adjmat[i].remove(i)\n",
    "    \n",
    "    #check if adacency matrix was populated\n",
    "    print(\"adjmat is \", adjmat)\n",
    "        \n",
    "    #Fill up with initial values\n",
    "    for i in range(n):\n",
    "      node_values.insert(i, random.randint(1, 9));\n",
    "    print(\"node values \",node_values)\n",
    "    \n",
    "    #inital conflicts\n",
    "    conf = conflicts_count(nodes,node_values,adjmat)\n",
    "\n",
    "    #min conflicts algorithm\n",
    "    for i in range(max_steps):\n",
    "      if len(conf) == 0:\n",
    "        #ANSWER FOUND!! \n",
    "        print(node_values)\n",
    "        return node_values\n",
    "      else:\n",
    "        iternode = random.choice(conf)\n",
    "        newval = conflicts_util(nodes, node_values, adjmat, iternode)\n",
    "        node_values[iternode]=newval\n",
    "        conf = conflicts_count(nodes, node_values, adjmat)\n",
    "            \n",
    "    print(\"Reached max_steps i.e. \",max_steps,\"Converge failed!\")\n",
    "    return node_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "pMkrU0RoyXGa",
    "outputId": "ace8342a-bf10-4f9c-dd2d-69ab8f1b8c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adjmat is  [{1, 2}, {0, 2, 3, 4}, {0, 1, 3, 4}, {1, 2}, {1, 2}]\n",
      "node values  [9, 3, 6, 8, 2]\n",
      "Reached max_steps i.e.  1000 Converge failed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[9, 2, 6, 8, 2]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is an exmaple input to test your code on. It is solveable.\n",
    "nodes = 'CHTPS'\n",
    "arcs = [(0,1), (0,2), (1,2), (1,3), (1,4), (2,3), (2,4)]\n",
    "max_steps = 1000\n",
    "solve_csp(nodes, arcs, max_steps)\n",
    "\n",
    "# E.g. pass test case: input\n",
    "# adjmat is  [{1, 2}, {0, 2, 3, 4}, {0, 1, 3, 4}, {1, 2}, {1, 2}]\n",
    "# node values  [4, 9, 1, 8, 5]\n",
    "# Output:\n",
    "# [4, 1, 2, 3, 2]\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "BvSAcrFByXGb",
    "nbgrader": {
     "checksum": "2b5a3fffc0a978cbe3582ae78155d159",
     "grade": false,
     "grade_id": "cell-a64a181856d55be5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "mzMqppXoyXGc",
    "nbgrader": {
     "checksum": "37cec1e95737276e8d5c179c3dbbad49",
     "grade": false,
     "grade_id": "cell-972dc9abc3181961",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Solve the following  MDP using both value iteration and policy iteration, you can do this by hand or programmatically, but you need to show your work in either case. \n",
    "\n",
    "There is a self-driving taxi that takes from place to place. Its goal is to make the most money possible and it makes the most money in a particular town, MoneyTown. The taxi has a tendency to take routes that take it to different towns and it costs money for the taxi to drive from place to place.  \n",
    "\n",
    "There are three states that the taxi can be in: 'In MoneyTown', 'MoneyTown Suburbs', and 'Outside MoneyTown'. There are two actions that the taxi can take in each state: drive and wait. Driving costs \\$10. When the taxi is in money town it makes \\$30, in MoneyTown Suburbs and Outside MoneyTown it only makes \\$10. The reward for the taxi is:\n",
    "\n",
    "(money made - cost) \n",
    "\n",
    "For example if the taxi is driving around in MoneyTown, the reward is \\$30-\\$10=\\$20.\n",
    "\n",
    "If the taxi is in MoneyTown and drives, then it is still MoneyTown in the next period with probability .9, and in the MoneyTown Suburbs in the next period with probability .1. If it is MoneyTown and does not drive, these probabilities are .7 and .3, respectively. If it is in the MoneyTown Suburbs and drives, then with probability .3 it is in MoneyTown in the next period, with probability .6 it is still in MoneyTown Suburbs in the next period, and with probability .1 it is in Outside MoneyTown in the next period. If it is in MoneyTown Suburbs and does not drive, then with probability 1 it is Outside MoneyTown next period. Finally, if it is in Outside MoneyTown and drives, then in the next period it is in MoneyTown with probability .6, and at the OutSide MoneyTown with probability .4. If it does not drive, then with probability 1 it is at Outside MoneyTown in the next period. \n",
    "\n",
    "1. Draw the MDP graphically\n",
    "  - A good way to do this is through [Google Drawings](https://docs.google.com/drawings)\n",
    "  - When you're done you can embed it in the jupyter notebook using markdown syntax\n",
    "  - \\!\\[alt-text\\]\\(url\\)\n",
    "  - To get the URL for your image in Google Draw goto File->Publish to the web...->Embed and copy the src portion of the html tag\n",
    "  \n",
    "2. Using a discount factor of .8, solve the MDP using value iteration (until the values have become reasonably stable). You should start with the values set to zero. You should show both the optimal policy and the optimal values.\n",
    "3. Using a discount factor of .8, solve the MDP using policy iteration (until you have complete convergence). You should start with the policy that never drives. Again, you should show both the optimal policy and the optimal values (and of course they should be the same as in 2...).\n",
    "4. Change the MDP in three different ways: by changing the discount factor, changing the transition probabilities for a single action from a single state, and by changing a reward for a single action at a single state. Each of these changes should be performed separately starting at the original MDP, resulting in three new MDPs (which you do not have to draw), each of which is different from the original MDP in a single way. In each case, the change should be so that the optimal policy changes, and you should state what the optimal policy becomes and give a short intuitive argument for this.\n",
    "\n",
    "\n",
    "**If you solve the problem programmatically, put your code in here. If you solve it by hand include your work here as well. You can add cells as you feel the need.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8h8UNCbgWGSa"
   },
   "source": [
    "## Question 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "colab_type": "code",
    "id": "LlyBWSqlrXpf",
    "outputId": "35c083f0-0536-43c6-c144-5f55fc441dda"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSV1qVR1nr-l45prl_fYbX8d2Z7M-GDvu5jiniNBnj8xxylRL-WfOyXOetBCq0mkfzZ450dCLQtvBdm/pub?w=960&amp;h=720\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vSV1qVR1nr-l45prl_fYbX8d2Z7M-GDvu5jiniNBnj8xxylRL-WfOyXOetBCq0mkfzZ450dCLQtvBdm/pub?w=960&amp;h=720\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIwYTayaV-lL"
   },
   "source": [
    "##Question 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6jH-mRLgNm27",
    "outputId": "5f2dce74-60f1-4156-f169-c29d91591ae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0]\n",
      "By policy iteration Optimal policy is [wait wait drive ] with Optimal Values are =  [97.8361669242658, 79.36630602782068, 86.70788253477585]\n"
     ]
    }
   ],
   "source": [
    "#given\n",
    "gamma = 0.8 #discount factor\n",
    "#0 intown 1 suburb 2 outoftown\n",
    "states = [0,1,2]\n",
    "#print(len(states))\n",
    "#0 drive 1 wait\n",
    "actions = [0,1]\n",
    "#print(len(actions))\n",
    "Trans = numpy.zeros((3, 2, 3))\n",
    "Reward = numpy.zeros((3, 2, 3))\n",
    "V = [0, 0 ,0]     #expected utility\n",
    "Qopt = 0\n",
    "#---------\n",
    "Trans[0,0,0] = 0.9\n",
    "Trans[0,0,1] = 0.1\n",
    "Trans[0,1,0] = 0.7\n",
    "Trans[0,1,1] = 0.3\n",
    "#---------\n",
    "Trans[1,0,0] = 0.3\n",
    "Trans[1,0,1] = 0.6\n",
    "Trans[1,0,2] = 0.1\n",
    "Trans[1,1,2] = 1\n",
    "#---------\n",
    "Trans[2,0,2] = 0.4\n",
    "Trans[2,0,0] = 0.6\n",
    "Trans[2,1,2] = 1\n",
    "#---------\n",
    "Reward[0,0,0] = 20\n",
    "Reward[0,0,1] = 0\n",
    "Reward[0,1,0] = 30\n",
    "Reward[0,1,1] = 10\n",
    "#---------\n",
    "Reward[1,0,0] = 20\n",
    "Reward[1,0,1] = 0\n",
    "Reward[1,0,2] = 0\n",
    "Reward[1,1,2] = 10\n",
    "#---------\n",
    "Reward[2,0,2] = 0\n",
    "Reward[2,0,0] = 20\n",
    "Reward[2,1,2] = 10\n",
    "#---------\n",
    "pie = [0, 0, 0]   #random policy - always drive\n",
    "iteration = 0\n",
    "\n",
    "def policy_evaluation():\n",
    "  for state in states:\n",
    "    Vpie = 0\n",
    "    for state1 in states:\n",
    "      Vpie += Trans[state, pie[state], state1] * (Reward[state, pie[state], state1] + (gamma * V[state1] ))\n",
    "    V[state] = Vpie\n",
    "  return V\n",
    "\n",
    "#policy iteration\n",
    "unchanged = True\n",
    "while unchanged:  #do\n",
    "  unchanged = False\n",
    "  V = policy_evaluation();\n",
    "  #print(V)\n",
    "  #improvement phase \n",
    "  for state in states:\n",
    "    Qopt = V[state]\n",
    "    for action in actions:\n",
    "      Qs = 0\n",
    "      for state1 in states:\n",
    "        Qs += Trans[state, action, state1] * (Reward[state, action, state1] + (gamma * V[state1] ))\n",
    "        if (Qs > Qopt):\n",
    "          pie[state] = action\n",
    "          Qopt = Qs\n",
    "          unchanged = True\n",
    "        \n",
    "\n",
    "#result\n",
    "print (pie)\n",
    "print(\"By policy iteration Optimal policy is [\",end=\"\")\n",
    "for action in pie:\n",
    "  if action == 0:\n",
    "    print(\"drive \",end=\"\")\n",
    "  else:\n",
    "    print(\"wait \",end=\"\")\n",
    "print(\"] with Optimal Values are = \", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0G42FHF-Vzbl"
   },
   "source": [
    "## **Question 2.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "V4GCbQelNq_R",
    "outputId": "d01eefcd-0d82-4364-900d-db63c36bd316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value is  [84.86965778887983, 72.04400648627463, 77.5550285303628]\n",
      "Optimal policy is  [1, 1, 0]\n",
      "By value iteration Optimal policy is [wait wait drive ]\n"
     ]
    }
   ],
   "source": [
    "#value iteration\n",
    "Uopt = [0, 0 ,0]     #expected utility \n",
    "epsilon = 0.0001     # error allowed\n",
    "iteration = 0\n",
    "def expected_reward(state, action):\n",
    "  result = 0\n",
    "  for dest in states:\n",
    "    result += Reward[state, action, dest] * Trans[state, action, dest]\n",
    "  return result\n",
    "\n",
    "while True:\n",
    "\n",
    "  delta = 0\n",
    "  #outdate the optimal into prev_U\n",
    "  prev_U = Uopt.copy()\n",
    "  for state in states:\n",
    "    Vmax = 0\n",
    "    state1max = 0\n",
    "    actionmax = 0\n",
    "    #find expected value of each action\n",
    "    for action in actions:\n",
    "      V = 0\n",
    "      for state1 in states:\n",
    "        V += Trans[state, action, state1] * prev_U[state1]\n",
    "        if (V > Vmax):\n",
    "          Vmax = V\n",
    "          actionmax = action\n",
    "\n",
    "    Uopt[state] = expected_reward(state, actionmax) + (gamma * Vmax)\n",
    "    \n",
    "    delta = max(delta, abs(Uopt[state] - prev_U[state])) \n",
    "  #exit if u find change to be too less  \n",
    "  if (delta < epsilon * (1 - gamma) / gamma):\n",
    "    break\n",
    "\n",
    "print(\"Optimal value is \", Uopt)\n",
    "\n",
    "#get policy from Uopt or utility function\n",
    "valiterpie = [0,0,0]\n",
    "for state in states:\n",
    "  UofA = [0,0]\n",
    "  for action in actions:\n",
    "    for state1 in states:\n",
    "      UofA[action] += Trans[state,action,state1] * (Reward[state,action,state1] + (gamma * Uopt[state1]))\n",
    "  valiterpie[state] = numpy.argmax(UofA)\n",
    "  \n",
    "print(\"Optimal policy is \", valiterpie)\n",
    "\n",
    "#result\n",
    "print(\"By value iteration Optimal policy is [\",end=\"\")\n",
    "for i in valiterpie:\n",
    "  if i == 0:\n",
    "    print(\"drive \",end=\"\")\n",
    "  else:\n",
    "    print(\"wait \",end=\"\")\n",
    "print(\"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYVEMc1agYVf"
   },
   "source": [
    "**Comment : **The policy from value iteration and policy iteration are seen to be the same i.e [wait wait drive ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvlAZ8F5WPj4"
   },
   "source": [
    "## Question 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1D99M0LWUCZ"
   },
   "source": [
    "## Question 2.4.1 - Discount factor, Gamma changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MA5RH7baWaTX",
    "outputId": "9c20bc44-4551-4da9-c1c6-8dac4f604d6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0]\n",
      "By policy iteration Optimal policy is [wait wait drive ] with Optimal Values are =  [1839.4197980727738, 1820.5450437991308, 1828.8333775748797]\n"
     ]
    }
   ],
   "source": [
    "#given\n",
    "gamma = 0.99 #discount factor\n",
    "#0 intown 1 suburb 2 outoftown\n",
    "states = [0,1,2]\n",
    "#print(len(states))\n",
    "#0 drive 1 wait\n",
    "actions = [0,1]\n",
    "#print(len(actions))\n",
    "Trans = numpy.zeros((3, 2, 3))\n",
    "Reward = numpy.zeros((3, 2, 3))\n",
    "V = [0, 0 ,0]     #expected utility\n",
    "Qopt = 0\n",
    "#---------\n",
    "Trans[0,0,0] = 0.9\n",
    "Trans[0,0,1] = 0.1\n",
    "Trans[0,1,0] = 0.7\n",
    "Trans[0,1,1] = 0.3\n",
    "#---------\n",
    "Trans[1,0,0] = 0.3\n",
    "Trans[1,0,1] = 0.6\n",
    "Trans[1,0,2] = 0.1\n",
    "Trans[1,1,2] = 1\n",
    "#---------\n",
    "Trans[2,0,2] = 0.4\n",
    "Trans[2,0,0] = 0.6\n",
    "Trans[2,1,2] = 1\n",
    "#---------\n",
    "Reward[0,0,0] = 20\n",
    "Reward[0,0,1] = 0\n",
    "Reward[0,1,0] = 30\n",
    "Reward[0,1,1] = 10\n",
    "#---------\n",
    "Reward[1,0,0] = 20\n",
    "Reward[1,0,1] = 0\n",
    "Reward[1,0,2] = 0\n",
    "Reward[1,1,2] = 10\n",
    "#---------\n",
    "Reward[2,0,2] = 0\n",
    "Reward[2,0,0] = 20\n",
    "Reward[2,1,2] = 10\n",
    "#---------\n",
    "pie = [0, 0, 0]   #random policy - always drive\n",
    "iteration = 0\n",
    "\n",
    "def policy_evaluation():\n",
    "  for state in states:\n",
    "    Vpie = 0\n",
    "    for state1 in states:\n",
    "      Vpie += Trans[state, pie[state], state1] * (Reward[state, pie[state], state1] + (gamma * V[state1] ))\n",
    "    V[state] = Vpie\n",
    "  return V\n",
    "\n",
    "#policy iteration\n",
    "unchanged = True\n",
    "while unchanged:  #do\n",
    "  unchanged = False\n",
    "  V = policy_evaluation();\n",
    "  #print(V)\n",
    "  #improvement phase \n",
    "  for state in states:\n",
    "    Qopt = V[state]\n",
    "    for action in actions:\n",
    "      Qs = 0\n",
    "      for state1 in states:\n",
    "        Qs += Trans[state, action, state1] * (Reward[state, action, state1] + (gamma * V[state1] ))\n",
    "        if (Qs > Qopt):\n",
    "          pie[state] = action\n",
    "          Qopt = Qs\n",
    "          unchanged = True\n",
    "        \n",
    "\n",
    "#result\n",
    "print (pie)\n",
    "print(\"By policy iteration Optimal policy is [\",end=\"\")\n",
    "for action in pie:\n",
    "  if action == 0:\n",
    "    print(\"drive \",end=\"\")\n",
    "  else:\n",
    "    print(\"wait \",end=\"\")\n",
    "print(\"] with Optimal Values are = \", V)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zr00X8y_YlCJ"
   },
   "source": [
    "**Policy Iteration output**\n",
    "\n",
    "Output for Gamma = 0.99\n",
    "\n",
    "[1, 1, 0]\n",
    "By policy iteration Optimal policy is [wait wait drive ] with Optimal Values are =  [1839.4197980727738, 1820.5450437991308, 1828.8333775748797]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6XRv2_IwYNw1"
   },
   "source": [
    "Output for Gamma = 0.5\n",
    "\n",
    "[1, 1, 0]\n",
    "By policy iteration Optimal policy is [wait wait drive ] with Optimal Values are =  [42.81407035175878, 25.527638190954768, 31.055276381909536]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSB1wzX-YCR-"
   },
   "source": [
    "Output for Gamma = 0.1\n",
    "\n",
    "[1, 1, 0]\n",
    "By policy iteration Optimal policy is [wait wait drive ] with Optimal Values are =  [26.174631982254486, 11.41359144988909, 14.135914498890903]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8YeJCwglXz_g"
   },
   "source": [
    "Output for Gamma = 0.01 \n",
    "\n",
    "[1, 1, 0]\n",
    "By policy iteration Optimal policy is [wait wait drive ] with Optimal Values are =  [24.19976416841338, 10.121939744829422, 12.193974482942249]\n",
    "\n",
    "\n",
    "\n",
    "**Intuitive Argument:** This result makes sense because, this is an infinite MDP and optimal policy is obtained not based on a final goal. So collecting the rewards immediately or later does not matter which is the soul purpose of the discount factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BoIC5Bl0cDQu"
   },
   "source": [
    "## **Question 2.4.2 Transition probability changed** Trans[1,0,0] = 0.98 (balance other probabilties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "FzZQqE8aY-Le",
    "outputId": "79bc32d4-941a-4a06-ae63-7a44123dcb47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "By policy iteration Optimal policy is [wait drive drive ] with Optimal Values are =  [115.5868902439024, 111.90929878048777, 99.23780487804875]\n"
     ]
    }
   ],
   "source": [
    "#given\n",
    "gamma = 0.8 #discount factor\n",
    "#0 intown 1 suburb 2 outoftown\n",
    "states = [0,1,2]\n",
    "#print(len(states))\n",
    "#0 drive 1 wait\n",
    "actions = [0,1]\n",
    "#print(len(actions))\n",
    "Trans = numpy.zeros((3, 2, 3))\n",
    "Reward = numpy.zeros((3, 2, 3))\n",
    "V = [0, 0 ,0]     #expected utility\n",
    "Qopt = 0\n",
    "#---------\n",
    "Trans[0,0,0] = 0.9\n",
    "Trans[0,0,1] = 0.1\n",
    "Trans[0,1,0] = 0.7\n",
    "Trans[0,1,1] = 0.3\n",
    "#---------\n",
    "Trans[1,0,0] = 0.98\n",
    "Trans[1,0,1] = 0.01\n",
    "Trans[1,0,2] = 0.01\n",
    "Trans[1,1,2] = 1\n",
    "#---------\n",
    "Trans[2,0,2] = 0.4\n",
    "Trans[2,0,0] = 0.6\n",
    "Trans[2,1,2] = 1\n",
    "#---------\n",
    "Reward[0,0,0] = 20\n",
    "Reward[0,0,1] = 0\n",
    "Reward[0,1,0] = 30\n",
    "Reward[0,1,1] = 10\n",
    "#---------\n",
    "Reward[1,0,0] = 20\n",
    "Reward[1,0,1] = 0\n",
    "Reward[1,0,2] = 0\n",
    "Reward[1,1,2] = 10\n",
    "#---------\n",
    "Reward[2,0,2] = 0\n",
    "Reward[2,0,0] = 20\n",
    "Reward[2,1,2] = 10\n",
    "#---------\n",
    "pie = [0, 0, 0]   #random policy - always drive\n",
    "iteration = 0\n",
    "\n",
    "def policy_evaluation():\n",
    "  for state in states:\n",
    "    Vpie = 0\n",
    "    for state1 in states:\n",
    "      Vpie += Trans[state, pie[state], state1] * (Reward[state, pie[state], state1] + (gamma * V[state1] ))\n",
    "    V[state] = Vpie\n",
    "  return V\n",
    "\n",
    "#policy iteration\n",
    "unchanged = True\n",
    "while unchanged:  #do\n",
    "  unchanged = False\n",
    "  V = policy_evaluation();\n",
    "  #print(V)\n",
    "  #improvement phase \n",
    "  for state in states:\n",
    "    Qopt = V[state]\n",
    "    for action in actions:\n",
    "      Qs = 0\n",
    "      for state1 in states:\n",
    "        Qs += Trans[state, action, state1] * (Reward[state, action, state1] + (gamma * V[state1] ))\n",
    "        if (Qs > Qopt):\n",
    "          pie[state] = action\n",
    "          Qopt = Qs\n",
    "          unchanged = True\n",
    "        \n",
    "\n",
    "#result\n",
    "print (pie)\n",
    "print(\"By policy iteration Optimal policy is [\",end=\"\")\n",
    "for action in pie:\n",
    "  if action == 0:\n",
    "    print(\"drive \",end=\"\")\n",
    "  else:\n",
    "    print(\"wait \",end=\"\")\n",
    "print(\"] with Optimal Values are = \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_bBEmTOje7kr"
   },
   "source": [
    "\n",
    "[1, 0, 0]\n",
    "\n",
    "By policy iteration Optimal policy is [wait drive drive ] with Optimal Values are =  [115.5868902439024, 111.90929878048777, 99.23780487804875]\n",
    "\n",
    "\n",
    "**Intuitive Argument:** This makes sense because we give so much high probability(0.98) that moving from state 1 to 0 will happen that the agent decides to take the drive and the bet for a high reward safely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHCtk2piceZD"
   },
   "source": [
    "## **Question 2.4.3 Rewards changed Reward[1,0,2] = 80**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "_uQuoT6qccK8",
    "outputId": "5ac45798-5d4c-4712-e420-cd55cd9acf5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0]\n",
      "By policy iteration Optimal policy is [wait drive drive ] with Optimal Values are =  [16007.196480298095, 15991.206077736222, 15995.191691168802]\n"
     ]
    }
   ],
   "source": [
    "#given\n",
    "gamma = 0.9988 #discount factor\n",
    "#0 intown 1 suburb 2 outoftown\n",
    "states = [0,1,2]\n",
    "#print(len(states))\n",
    "#0 drive 1 wait\n",
    "actions = [0,1]\n",
    "#print(len(actions))\n",
    "Trans = numpy.zeros((3, 2, 3))\n",
    "Reward = numpy.zeros((3, 2, 3))\n",
    "V = [0, 0 ,0]     #expected utility\n",
    "Qopt = 0\n",
    "#---------\n",
    "Trans[0,0,0] = 0.9\n",
    "Trans[0,0,1] = 0.1\n",
    "Trans[0,1,0] = 0.7\n",
    "Trans[0,1,1] = 0.3\n",
    "#---------\n",
    "Trans[1,0,0] = 0.3\n",
    "Trans[1,0,1] = 0.6\n",
    "Trans[1,0,2] = 0.1\n",
    "Trans[1,1,2] = 1\n",
    "#---------\n",
    "Trans[2,0,2] = 0.4\n",
    "Trans[2,0,0] = 0.6\n",
    "Trans[2,1,2] = 1\n",
    "#---------\n",
    "Reward[0,0,0] = 20\n",
    "Reward[0,0,1] = 0\n",
    "Reward[0,1,0] = 30\n",
    "Reward[0,1,1] = 10\n",
    "#---------\n",
    "Reward[1,0,0] = 20\n",
    "Reward[1,0,1] = 0\n",
    "Reward[1,0,2] = 80\n",
    "Reward[1,1,2] = 10\n",
    "#---------\n",
    "Reward[2,0,2] = 0\n",
    "Reward[2,0,0] = 20\n",
    "Reward[2,1,2] = 10\n",
    "#---------\n",
    "pie = [0, 0, 0]   #random policy - always drive\n",
    "iteration = 0\n",
    "\n",
    "def policy_evaluation():\n",
    "  for state in states:\n",
    "    Vpie = 0\n",
    "    for state1 in states:\n",
    "      Vpie += Trans[state, pie[state], state1] * (Reward[state, pie[state], state1] + (gamma * V[state1] ))\n",
    "    V[state] = Vpie\n",
    "  return V\n",
    "\n",
    "#policy iteration\n",
    "unchanged = True\n",
    "while unchanged:  #do\n",
    "  unchanged = False\n",
    "  V = policy_evaluation();\n",
    "  #print(V)\n",
    "  #improvement phase \n",
    "  for state in states:\n",
    "    Qopt = V[state]\n",
    "    for action in actions:\n",
    "      Qs = 0\n",
    "      for state1 in states:\n",
    "        Qs += Trans[state, action, state1] * (Reward[state, action, state1] + (gamma * V[state1] ))\n",
    "        if (Qs > Qopt):\n",
    "          pie[state] = action\n",
    "          Qopt = Qs\n",
    "          unchanged = True\n",
    "        \n",
    "\n",
    "#result\n",
    "print (pie)\n",
    "print(\"By policy iteration Optimal policy is [\",end=\"\")\n",
    "for action in pie:\n",
    "  if action == 0:\n",
    "    print(\"drive \",end=\"\")\n",
    "  else:\n",
    "    print(\"wait \",end=\"\")\n",
    "print(\"] with Optimal Values are = \", V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhD6tOqJdu6m"
   },
   "source": [
    "Reward[1,0,2] = 0\n",
    "\n",
    "[1, 1, 0]\n",
    "\n",
    "By policy iteration Optimal policy is** [wait wait drive ]** with Optimal Values are =  [97.8361669242658, 79.36630602782068, 86.70788253477585]\n",
    "\n",
    "Reward[1,0,2] = 80\n",
    "\n",
    "[1, 0, 0]\n",
    "\n",
    "By policy iteration Optimal policy is **[wait drive drive ]** with Optimal Values are =  [102.62686567164177, 88.14925373134328, 90.08955223880596]\n",
    "\n",
    "**Intuitive Argument:** This makes sense because we claim to give a better reward for waiting while on the move from 1 to 2. So the optimal state at state 1 changed from wait to drive to 2 so that the 80 is claimed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sOT3EaAgArB"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Shrbs-ssignment2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
